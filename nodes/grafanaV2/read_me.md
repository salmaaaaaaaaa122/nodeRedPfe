# Python Container Node for Node-RED

## üìã Table des mati√®res

- [Vue d'ensemble](#vue-densemble)
- [Architecture technique](#architecture-technique)
- [Installation et pr√©requis](#installation-et-pr√©requis)
- [Configuration](#configuration)
- [Utilisation](#utilisation)
- [API et contexte d'ex√©cution](#api-et-contexte-dex√©cution)
- [Gestion des erreurs](#gestion-des-erreurs)
- [Performance et optimisation](#performance-et-optimisation)
- [S√©curit√©](#s√©curit√©)
- [D√©pannage](#d√©pannage)
- [Exemples d'utilisation](#exemples-dutilisation)

## üéØ Vue d'ensemble

Le **Python Container Node** est un n≈ìud personnalis√© pour Node-RED qui permet l'ex√©cution de code Python arbitraire dans des conteneurs Docker isol√©s. Ce module r√©sout le probl√®me de l'ex√©cution s√©curis√©e et reproductible de scripts Python avec des d√©pendances sp√©cifiques dans un environnement Node.js.

### Probl√®me r√©solu

- **Isolation des d√©pendances** : Chaque script Python s'ex√©cute avec ses propres biblioth√®ques
- **S√©curit√©** : Isolation compl√®te du syst√®me h√¥te via Docker
- **Reproductibilit√©** : Environnements d'ex√©cution coh√©rents et versionn√©s
- **Interop√©rabilit√©** : Passage de donn√©es seamless entre Node-RED et Python

### Cas d'usage typiques

- Traitement de donn√©es scientifiques (NumPy, Pandas, SciPy)
- Apprentissage automatique et IA (TensorFlow, PyTorch, scikit-learn)
- Analyse d'images et vision par ordinateur (OpenCV, PIL)
- Traitement de donn√©es g√©ospatiales (GeoPandas, Shapely)
- APIs et int√©grations sp√©cialis√©es n√©cessitant des biblioth√®ques Python

## üèó Architecture technique

### Flux d'ex√©cution

```mermaid
graph TD
    A[Message entrant] --> B[Pr√©paration workspace]
    B --> C[G√©n√©ration fichiers config]
    C --> D[Construction image Docker]
    D --> E[Ex√©cution conteneur]
    E --> F[Lecture r√©sultats]
    F --> G[Nettoyage ressources]
    G --> H[Message sortant]
    
    I[Gestion erreurs] -.-> B
    I -.-> C
    I -.-> D
    I -.-> E
    I -.-> F
```

### Architecture des fichiers

```
/tmp/python-exec-{uuid}/
‚îú‚îÄ‚îÄ Dockerfile              # Image personnalis√©e
‚îú‚îÄ‚îÄ requirements.txt         # D√©pendances Python
‚îú‚îÄ‚îÄ script.py               # Code utilisateur + wrapper
‚îú‚îÄ‚îÄ input.json              # Donn√©es d'entr√©e
‚îî‚îÄ‚îÄ output.json             # R√©sultats d'ex√©cution
```

### Stack technologique

- **Runtime** : Node.js avec Node-RED framework
- **Containerisation** : Docker Engine
- **Ex√©cution Python** : Images Docker officielles Python
- **Communication** : JSON pour s√©rialisation des donn√©es
- **Isolation** : Conteneurs Docker √©ph√©m√®res

## üîß Installation et pr√©requis

### Pr√©requis syst√®me

1. **Docker Engine** >= 20.10.0
   ```bash
   # Installation Ubuntu/Debian
   curl -fsSL https://get.docker.com -o get-docker.sh
   sh get-docker.sh
   
   # V√©rification
   docker --version
   docker run hello-world
   ```

2. **Node-RED** >= 3.0.0
   ```bash
   npm install -g node-red
   ```

3. **Permissions Docker**
   ```bash
   # Ajouter l'utilisateur au groupe docker
   sudo usermod -aG docker $USER
   # Red√©marrer la session
   ```

### Installation du module

1. **Copier les fichiers dans le r√©pertoire Node-RED**
   ```
   ~/.node-red/nodes/
   ‚îú‚îÄ‚îÄ python-container.js     # Code backend
   ‚îî‚îÄ‚îÄ python-container.html   # Interface frontend
   ```

2. **Red√©marrer Node-RED**
   ```bash
   node-red-restart
   # ou
   pm2 restart node-red
   ```

3. **V√©rification de l'installation**
   - Le n≈ìud "Python Container" appara√Æt dans la palette "function"
   - Ic√¥ne bleue avec logo Python

## ‚öôÔ∏è Configuration

### Onglet Configuration

#### Nom du n≈ìud
- **Champ** : `name`
- **Type** : String (optionnel)
- **Description** : Nom affich√© dans le flow pour identification
- **Exemple** : "Analyse donn√©es capteurs"

#### Image Docker
- **Champ** : `containerImage`
- **Type** : String
- **Valeur par d√©faut** : `python:3.9-slim`
- **Description** : Image Docker de base pour l'environnement Python
- **Exemples** :
  ```
  python:3.9-slim          # L√©ger, recommand√©
  python:3.11              # Version compl√®te
  python:3.12-alpine       # Ultra-l√©ger
  jupyter/scipy-notebook   # Stack scientifique
  tensorflow/tensorflow    # Machine Learning
  ```

#### Biblioth√®ques Python
- **Champ** : `pythonLibraries`
- **Type** : Textarea
- **Format** : Une biblioth√®que par ligne ou s√©par√©es par virgules
- **Description** : D√©pendances Python √† installer via pip
- **Exemples** :
  ```
  requests
  pandas>=1.3.0
  numpy
  matplotlib
  scikit-learn==1.1.0
  ```

#### Timeout d'ex√©cution
- **Champ** : `timeout`
- **Type** : Number (ms)
- **Valeur par d√©faut** : 30000 (30 secondes)
- **Plage** : 1000-300000 ms
- **Description** : Temps maximum d'ex√©cution avant interruption

### Onglet Code Python

#### √âditeur de code
- **√âditeur** : ACE Editor avec syntaxe Python
- **Fonctionnalit√©s** :
  - Coloration syntaxique
  - Autocompl√©tion
  - V√©rification d'erreurs
  - Recherche/remplacement (Ctrl+F)
  - Pliage de code

#### Exemples pr√©d√©finis
- **Traitement simple** : Transformation basique de donn√©es
- **Traitement JSON** : Manipulation d'objets complexes
- **Traitement avanc√©** : Utilisation de pandas/numpy
- **Gestion de fichiers** : Lecture/√©criture de fichiers
- **Commandes syst√®me** : Ex√©cution de commandes shell

### Onglet Avanc√©

#### R√©utilisation de conteneurs
- **Champ** : `reuseContainer`
- **Type** : Boolean
- **Description** : Cache les images Docker pour am√©liorer les performances
- **Impact** : R√©duction du temps de construction de 5-30 secondes

#### Limites de ressources
- **M√©moire maximale** : `maxMemory` (ex: 512m, 1g, 2048m)
- **CPUs maximum** : `maxCpus` (ex: 1, 0.5, 2)
- **Description** : Pr√©vient la consommation excessive de ressources

#### Isolation r√©seau
- **Champ** : `networkIsolation`
- **Type** : Boolean
- **Valeur par d√©faut** : true
- **Description** : Bloque l'acc√®s r√©seau pour s√©curit√© renforc√©e

## üìù Utilisation

### Variables d'entr√©e disponibles

Le contexte d'ex√©cution Python dispose automatiquement des variables suivantes :

```python
# Variables principales
payload          # Donn√©es du message entrant (any type)
topic           # Sujet du message (string)
timestamp       # Horodatage ISO 8601 (string)
previous_result # Alias pour payload (any type)

# M√©tadonn√©es d'ex√©cution
execution_id    # ID unique d'ex√©cution (string)
node_id        # ID du n≈ìud Node-RED (string)
original_message # Propri√©t√©s compl√®tes du message (dict)
```

### Variables de sortie

Le script Python doit d√©finir les variables suivantes pour le r√©sultat :

```python
# Obligatoire
result = "valeur de sortie"  # Devient msg.payload

# Optionnel
output_topic = "nouveau/topic"    # Devient msg.topic
additional_data = {"key": "value"} # Ajout√© √† msg.additionalData
```

### Fonctions utilitaires

#### Journalisation
```python
log(message, level='info')
# Niveaux : 'info', 'warning', 'error', 'debug'
# Exemple :
log("Traitement d√©marr√©", "info")
log("Attention: donn√©es manquantes", "warning")
```

#### Gestion de fichiers
```python
# Sauvegarder un fichier
save_file("results.json", json.dumps(data))

# Lire un fichier
content = read_file("data.txt")
if content:
    lines = content.split('\n')
```

#### Commandes syst√®me
```python
# Ex√©cuter une commande
result = run_command("ls -la")
if result and result['returncode'] == 0:
    print(result['stdout'])
```

## üîå API et contexte d'ex√©cution

### Structure du message d'entr√©e

```javascript
{
  "payload": any,           // Donn√©es principales
  "topic": string,          // Sujet/canal
  "_msgid": string,         // ID unique du message
  "timestamp": string,      // Horodatage
  // ... autres propri√©t√©s personnalis√©es
}
```

### Structure du message de sortie

```javascript
{
  "payload": any,              // R√©sultat du script Python
  "topic": string,             // Topic original ou modifi√©
  "pythonOutput": string,      // stdout du script Python
  "pythonErrors": string,      // stderr du script Python
  "executionId": string,       // ID d'ex√©cution
  "executionTimestamp": string, // Timestamp de fin
  "additionalData": object     // Donn√©es suppl√©mentaires
}
```

### Cycle de vie d'ex√©cution

1. **Pr√©paration** (100-200ms)
   - Cr√©ation workspace temporaire
   - G√©n√©ration des fichiers de configuration
   - S√©rialisation des donn√©es d'entr√©e

2. **Construction** (2-30s selon cache)
   - Construction de l'image Docker personnalis√©e
   - Installation des d√©pendances Python
   - Optimisation des couches Docker

3. **Ex√©cution** (variable)
   - Lancement du conteneur
   - Ex√©cution du script Python
   - Capture des r√©sultats

4. **Finalisation** (50-100ms)
   - Lecture des fichiers de sortie
   - Nettoyage des ressources temporaires
   - Construction du message de sortie

## ‚ùå Gestion des erreurs

### Types d'erreurs

#### Erreurs de pr√©paration
```
Erreur de pr√©paration: ENOENT: no such file or directory
```
- **Cause** : Probl√®me de permissions ou d'espace disque
- **Solution** : V√©rifier les permissions `/tmp` et l'espace disponible

#### Erreurs de construction Docker
```
Erreur de construction: failed to solve with frontend dockerfile.v0
```
- **Causes** : Image de base invalide, d√©pendances introuvables
- **Solutions** : 
  - V√©rifier la disponibilit√© de l'image Docker
  - Tester la connectivit√© r√©seau
  - Valider la syntaxe des requirements

#### Erreurs d'ex√©cution Python
```
Erreur Python: NameError: name 'undefined_variable' is not defined
```
- **Cause** : Erreur dans le code Python utilisateur
- **Solution** : D√©boguer le code Python avec les logs

#### Timeout d'ex√©cution
```
Timeout d'ex√©cution atteint
```
- **Cause** : Script trop lent ou boucle infinie
- **Solutions** :
  - Augmenter le timeout
  - Optimiser le code Python
  - V√©rifier les boucles infinies

### Debugging

#### Activer les logs d√©taill√©s
```javascript
// Dans settings.js de Node-RED
logging: {
    console: {
        level: "debug",
        metrics: false,
        audit: false
    }
}
```

#### Analyser les logs Python
```python
import sys
print("Debug info", file=sys.stderr)  # Visible dans pythonErrors
log("Processing step 1")               # Avec timestamp
```

## üöÄ Performance et optimisation

### M√©triques de performance

| Op√©ration | Sans cache | Avec cache | Optimisation |
|-----------|------------|------------|--------------|
| Image simple (python:3.9-slim) | 3-5s | 0.1s | Cache Docker |
| Avec pandas/numpy | 15-30s | 0.1s | Cache + layers |
| Premi√®re ex√©cution | Variable | N/A | Pre-pull images |

### Optimisations recommand√©es

#### 1. R√©utilisation de conteneurs
```javascript
// Configuration recommand√©e
reuseContainer: true  // Active le cache
```

#### 2. Images optimis√©es
```dockerfile
# Pr√©f√©rer les images slim
python:3.9-slim      # 45MB vs 885MB pour python:3.9

# Images sp√©cialis√©es
jupyter/scipy-notebook   # Stack scientifique pr√©-install√©
tensorflow/tensorflow    # ML pr√©-configur√©
```

#### 3. Gestion des d√©pendances
```txt
# requirements.txt optimis√©
pandas>=1.3.0,<2.0.0    # Contraintes de version
numpy                    # Pas de version si stable
scikit-learn==1.1.0     # Version exacte si critique
```

#### 4. Code Python optimis√©
```python
# √âviter les imports co√ªteux si non utilis√©s
if processing_needed:
    import heavy_library

# Utiliser les fonctions vectoris√©es
result = np.array(data).sum()  # Plus rapide que sum(data)
```

### Monitoring des ressources

```bash
# Surveiller l'utilisation Docker
docker stats

# Nettoyer les images orphelines
docker system prune -f

# Surveiller l'espace disque /tmp
df -h /tmp
```

## üîí S√©curit√©

### Isolation et containment

#### Isolation des processus
- Chaque ex√©cution dans un conteneur √©ph√©m√®re
- Pas d'acc√®s au syst√®me de fichiers h√¥te
- Isolation des processus et namespaces

#### Limitations de ressources
```javascript
// Configuration s√©curis√©e
maxMemory: "512m",     // Limite m√©moire
maxCpus: "1",          // Limite CPU
networkIsolation: true // Pas d'acc√®s r√©seau
```

#### Utilisateur non-root
```dockerfile
# Dans le conteneur
RUN useradd -m -u 1000 pythonuser
USER pythonuser
```

### Bonnes pratiques s√©curitaires

#### 1. Validation des entr√©es
```python
# Valider les donn√©es d'entr√©e
if not isinstance(payload, (dict, list, str, int, float)):
    raise ValueError("Type de donn√©es non support√©")

# Sanitiser les cha√Ænes
import re
if isinstance(payload, str):
    clean_payload = re.sub(r'[^\w\s-]', '', payload)
```

#### 2. Gestion des secrets
```python
# JAMAIS hardcoder des secrets
# api_key = "sk-1234..."  # ‚ùå MAUVAIS

# Utiliser des variables d'environnement ou Node-RED context
api_key = os.environ.get('API_KEY')  # ‚úÖ BON
```

#### 3. Limitation des capacit√©s
```python
# √âviter les op√©rations dangereuses
# os.system("rm -rf /")     # ‚ùå DANGEREUX
# subprocess.call([...])    # ‚ùå RISQU√â

# Utiliser run_command() qui est s√©curis√©e
result = run_command("safe_command")  # ‚úÖ BON
```

### Audit et monitoring

```bash
# Logs de s√©curit√© Docker
sudo journalctl -u docker

# Surveiller les conteneurs actifs
docker ps

# Audit des images
docker images --filter "dangling=true"
```

## üõ† D√©pannage

### Probl√®mes courants

#### Docker non accessible
**Sympt√¥me** :
```
Error: connect ENOENT /var/run/docker.sock
```
**Solutions** :
1. V√©rifier que Docker est d√©marr√© : `sudo systemctl status docker`
2. Permissions utilisateur : `sudo usermod -aG docker $USER`
3. Red√©marrer la session utilisateur

#### Manque d'espace disque
**Sympt√¥me** :
```
Error: no space left on device
```
**Solutions** :
1. Nettoyer Docker : `docker system prune -af`
2. Nettoyer /tmp : `sudo rm -rf /tmp/python-exec-*`
3. Augmenter l'espace disque

#### Timeout syst√©matique
**Sympt√¥me** :
```
Timeout d'ex√©cution atteint
```
**Diagnostic** :
```python
import time
start = time.time()
# Votre code ici
print(f"Dur√©e: {time.time() - start}s", file=sys.stderr)
```

#### Biblioth√®ques non trouv√©es
**Sympt√¥me** :
```
ModuleNotFoundError: No module named 'pandas'
```
**Solutions** :
1. V√©rifier requirements.txt
2. Tester l'installation : `pip install pandas`
3. Utiliser des versions compatibles

### Outils de diagnostic

#### Script de test
```python
# test_environment.py
import sys
import json
import os

print("=== Test environnement ===", file=sys.stderr)
print(f"Python version: {sys.version}", file=sys.stderr)
print(f"Working directory: {os.getcwd()}", file=sys.stderr)
print(f"Available files: {os.listdir('/app')}", file=sys.stderr)

# Test des variables
print(f"Payload type: {type(payload)}", file=sys.stderr)
print(f"Payload value: {payload}", file=sys.stderr)

# Test des fonctions
try:
    log("Test logging function")
    save_file("test.txt", "Hello World")
    content = read_file("test.txt")
    print(f"File test: {content}", file=sys.stderr)
except Exception as e:
    print(f"Function test error: {e}", file=sys.stderr)

result = "Environment test completed"
```

#### Monitoring avanc√©
```bash
#!/bin/bash
# monitor_node.sh
echo "=== Node-RED Python Container Monitor ==="
echo "Docker status: $(systemctl is-active docker)"
echo "Available images: $(docker images --format 'table {{.Repository}}:{{.Tag}}\t{{.Size}}')"
echo "Running containers: $(docker ps --format 'table {{.Names}}\t{{.Status}}')"
echo "Disk usage /tmp: $(du -sh /tmp/python-exec-* 2>/dev/null | wc -l) workspaces"
echo "Memory usage: $(free -h | grep Mem)"
```

## üìö Exemples d'utilisation

### Exemple 1 : Analyse de donn√©es IoT

**Contexte** : Traiter les donn√©es de capteurs avec pandas

```python
import pandas as pd
import numpy as np
from datetime import datetime

log("Analyse des donn√©es IoT d√©marr√©e")

# Conversion des donn√©es capteurs
if isinstance(payload, list):
    df = pd.DataFrame(payload)
    
    # Calculs statistiques
    stats = {
        'temperature_moyenne': df['temperature'].mean(),
        'humidite_max': df['humidity'].max(),
        'nb_mesures': len(df),
        'periode': f"{df['timestamp'].min()} √† {df['timestamp'].max()}"
    }
    
    # D√©tection d'anomalies
    temp_std = df['temperature'].std()
    temp_mean = df['temperature'].mean()
    anomalies = df[
        (df['temperature'] > temp_mean + 2*temp_std) |
        (df['temperature'] < temp_mean - 2*temp_std)
    ]
    
    result = {
        'statistiques': stats,
        'anomalies': anomalies.to_dict('records'),
        'statut': 'analys√©'
    }
    
    # Sauvegarde pour historique
    save_file('analyse_iot.json', pd.DataFrame([stats]).to_json())
    
    log(f"Analyse termin√©e: {len(anomalies)} anomalies d√©tect√©es")
    
else:
    result = {'erreur': 'Format de donn√©es invalide'}
    log("Erreur: donn√©es non conformes", "error")

output_topic = f"iot/analysis/{topic}"
```

### Exemple 2 : Vision par ordinateur

**Contexte** : Analyse d'images avec OpenCV

```python
import cv2
import numpy as np
import base64
import json

log("Analyse d'image d√©marr√©e")

try:
    # D√©coder l'image base64
    if isinstance(payload, dict) and 'image_data' in payload:
        image_data = base64.b64decode(payload['image_data'])
        
        # Sauvegarder l'image temporairement
        save_file('input_image.jpg', image_data)
        
        # Charger avec OpenCV
        image = cv2.imread('/app/input_image.jpg')
        
        if image is not None:
            # D√©tection de contours
            gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
            blurred = cv2.GaussianBlur(gray, (5, 5), 0)
            edges = cv2.Canny(blurred, 50, 150)
            
            # Trouver les contours
            contours, _ = cv2.findContours(edges, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)
            
            # Analyser les objets
            objects = []
            for i, contour in enumerate(contours):
                area = cv2.contourArea(contour)
                if area > 100:  # Filtrer les petits objets
                    x, y, w, h = cv2.boundingRect(contour)
                    objects.append({
                        'id': i,
                        'position': {'x': int(x), 'y': int(y)},
                        'taille': {'width': int(w), 'height': int(h)},
                        'surface': float(area)
                    })
            
            # Dessiner les r√©sultats
            result_image = image.copy()
            for obj in objects:
                x, y = obj['position']['x'], obj['position']['y']
                w, h = obj['taille']['width'], obj['taille']['height']
                cv2.rectangle(result_image, (x, y), (x+w, y+h), (0, 255, 0), 2)
            
            # Encoder l'image r√©sultat
            _, buffer = cv2.imencode('.jpg', result_image)
            result_image_b64 = base64.b64encode(buffer).decode()
            
            result = {
                'objets_detectes': len(objects),
                'details_objets': objects,
                'image_analysee': result_image_b64,
                'resolution': {'width': image.shape[1], 'height': image.shape[0]}
            }
            
            save_file('analysis_result.json', json.dumps(result, indent=2))
            log(f"Analyse termin√©e: {len(objects)} objets d√©tect√©s")
            
        else:
            result = {'erreur': 'Image non valide'}
            log("Erreur: impossible de charger l'image", "error")
    else:
        result = {'erreur': 'Donn√©es image manquantes'}
        log("Erreur: format de donn√©es incorrect", "error")

except Exception as e:
    result = {'erreur': f'Erreur de traitement: {str(e)}'}
    log(f"Erreur inattendue: {e}", "error")

output_topic = f"vision/analysis/{topic}"
additional_data = {
    'algorithme': 'opencv_contours',
    'version': '4.5.0'
}
```

### Exemple 3 : Machine Learning avec scikit-learn

**Contexte** : Classification de donn√©es avec apprentissage automatique

```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, accuracy_score
import joblib
import json

log("Entra√Ænement du mod√®le ML d√©marr√©")

try:
    # Charger les donn√©es d'entra√Ænement
    if isinstance(payload, dict) and 'training_data' in payload:
        df = pd.DataFrame(payload['training_data'])
        
        # Pr√©parer les features et targets
        feature_columns = [col for col in df.columns if col != 'target']
        X = df[feature_columns]
        y = df['target']
        
        log(f"Dataset: {len(df)} √©chantillons, {len(feature_columns)} features")
        
        # Division train/test
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # Entra√Ænement du mod√®le
        model = RandomForestClassifier(
            n_estimators=100,
            random_state=42,
            max_depth=10
        )
        model.fit(X_train, y_train)
        
        # √âvaluation
        y_pred = model.predict(X_test)
        accuracy = accuracy_score(y_test, y_pred)
        report = classification_report(y_test, y_pred, output_dict=True)
        
        # Importance des features
        feature_importance = dict(zip(
            feature_columns,
            model.feature_importances_.tolist()
        ))
        
        # Sauvegarder le mod√®le
        joblib.dump(model, '/app/trained_model.pkl')
        
        # Pr√©dictions sur nouvelles donn√©es si fournies
        predictions = []
        if 'prediction_data' in payload:
            pred_df = pd.DataFrame(payload['prediction_data'])
            pred_results = model.predict(pred_df[feature_columns])
            pred_proba = model.predict_proba(pred_df[feature_columns])
            
            for i, (pred, proba) in enumerate(zip(pred_results, pred_proba)):
                predictions.append({
                    'sample_id': i,
                    'prediction': int(pred),
                    'confidence': float(max(proba)),
                    'probabilities': proba.tolist()
                })
        
        result = {
            'model_performance': {
                'accuracy': float(accuracy),
                'detailed_report': report,
                'feature_importance': feature_importance
            },
            'training_info': {
                'samples_train': len(X_train),
                'samples_test': len(X_test),
                'features_used': feature_columns
            },
            'predictions': predictions
        }
        
        # Sauvegarder les r√©sultats
        save_file('ml_results.json', json.dumps(result, indent=2))
        save_file('model_info.txt', f"Accuracy: {accuracy:.3f}\nFeatures: {feature_columns}")
        
        log(f"Entra√Ænement termin√© - Accuracy: {accuracy:.3f}")
        
    else:
        result = {'erreur': 'Donn√©es d\'entra√Ænement manquantes'}
        log("Erreur: format de donn√©es incorrect", "error")

except Exception as e:
    result = {'erreur': f'Erreur ML: {str(e)}'}
    log(f"Erreur lors de l'entra√Ænement: {e}", "error")

output_topic = f"ml/model/{topic}"
additional_data = {
    'algorithm': 'RandomForest',
    'sklearn_version': '1.1.0',
    'model_saved': '/app/trained_model.pkl'
}
```

### Exemple 4 : Int√©gration API et web scraping

**Contexte** : R√©cup√©ration et traitement de donn√©es externes

```python
import requests
import json
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime
import time

log("R√©cup√©ration de donn√©es externes d√©marr√©e")

try:
    # Configuration des APIs depuis le payload
    config = payload.get('api_config', {})
    base_url = config.get('base_url', 'https://api.example.com')
    api_key = config.get('api_key', '')
    
    collected_data = []
    
    # 1. R√©cup√©ration via API REST
    if 'api_endpoints' in config:
        for endpoint in config['api_endpoints']:
            headers = {'Authorization': f'Bearer {api_key}'}
            
            log(f"Appel API: {endpoint}")
            response = requests.get(f"{base_url}/{endpoint}", headers=headers, timeout=10)
            
            if response.status_code == 200:
                api_data = response.json()
                collected_data.extend(api_data.get('data', []))
                log(f"API {endpoint}: {len(api_data.get('data', []))} √©l√©ments r√©cup√©r√©s")
            else:
                log(f"Erreur API {endpoint}: {response.status_code}", "warning")
            
            time.sleep(1)  # Rate limiting
    
    # 2. Web scraping si configur√©
    if 'scraping_urls' in config:
        for url_config in config['scraping_urls']:
            url = url_config['url']
            selector = url_config.get('selector', 'p')
            
            log(f"Scraping: {url}")
            response = requests.get(url, timeout=10)
            
            if response.status_code == 200:
                soup = BeautifulSoup(response.content, 'html.parser')
                elements = soup.select(selector)
                
                scraped_data = []
                for elem in elements[:10]:  # Limiter √† 10 √©l√©ments
                    scraped_data.append({
                        'text': elem.get_text().strip(),
                        'tag': elem.name,
                        'url_source': url
                    })
                
                collected_data.extend(scraped_data)
                log(f"Scraping {url}: {len(scraped_data)} √©l√©ments extraits")
            
            time.sleep(2)  # Rate limiting pour scraping
    
    # 3. Traitement et analyse des donn√©es collect√©es
    if collected_data:
        df = pd.DataFrame(collected_data)
        
        # Nettoyage des donn√©es
        if 'text' in df.columns:
            df['text_length'] = df['text'].str.len()
            df['word_count'] = df['text'].str.split().str.len()
        
        # Statistiques descriptives
        stats = {
            'total_records': len(df),
            'data_sources': len(config.get('api_endpoints', [])) + len(config.get('scraping_urls', [])),
            'collection_timestamp': datetime.now().isoformat()
        }
        
        # Ajout de statistiques sp√©cifiques selon les colonnes
        if 'text_length' in df.columns:
            stats.update({
                'avg_text_length': float(df['text_length'].mean()),
                'max_text_length': int(df['text_length'].max()),
                'total_words': int(df['word_count'].sum())
            })
        
        # Sauvegarde des donn√©es
        save_file('collected_data.json', df.to_json(orient='records', indent=2))
        save_file('collection_stats.json', json.dumps(stats, indent=2))
        
        result = {
            'status': 'success',
            'data': collected_data[:100],  # Limiter la sortie
            'statistics': stats,
            'full_data_available': len(collected_data) > 100
        }
        
        log(f"Collection termin√©e: {len(collected_data)} √©l√©ments trait√©s")
        
    else:
        result = {
            'status': 'no_data',
            'message': 'Aucune donn√©e collect√©e',
            'configuration': config
        }
        log("Aucune donn√©e collect√©e", "warning")

except requests.RequestException as e:
    result = {'erreur': f'Erreur r√©seau: {str(e)}'}
    log(f"Erreur de requ√™te: {e}", "error")

except Exception as e:
    result = {'erreur': f'Erreur de traitement: {str(e)}'}
    log(f"Erreur inattendue: {e}", "error")

output_topic = f"data/external/{topic}"
additional_data = {
    'collection_method': 'api_scraping',
    'libraries_used': ['requests', 'beautifulsoup4', 'pandas'],
    'rate_limited': True
}
```

## üîß Configuration Node-RED recommand√©e

### settings.js optimal

```javascript
module.exports = {
    // Timeout pour les n≈ìuds
    functionGlobalContext: {
        // Variables globales disponibles
    },
    
    // Configuration des logs
    logging: {
        console: {
            level: "info",
            metrics: false,
            audit: false
        },
        file: {
            level: "info",
            filename: "/var/log/node-red/node-red.log",
            maxFiles: 5,
            maxSize: "10MB"
        }
    },
    
    // Limites de performance
    runtimeState: {
        enabled: false,
        ui: false
    },
    
    // S√©curit√©
    adminAuth: {
        type: "credentials",
        users: [{
            username: "admin",
            password: "$2b$08$...",  // Hash bcrypt
            permissions: "*"
        }]
    },
    
    // Optimisations pour conteneurs
    editorTheme: {
        projects: {
            enabled: false
        }
    }
}
```

### Monitoring et maintenance

#### Script de maintenance automatique

```bash
#!/bin/bash
# maintain_python_containers.sh

echo "=== Maintenance Python Container Node ==="
date

# Nettoyer les workspaces anciens (> 1 jour)
find /tmp -name "python-exec-*" -type d -mtime +1 -exec rm -rf {} + 2>/dev/null
echo "Workspaces nettoy√©s: $(find /tmp -name "python-exec-*" -type d | wc -l) restants"

# Nettoyer les images Docker orphelines
docker image prune -f
echo "Images Docker nettoy√©es"

# Statistiques d'utilisation
echo "Conteneurs actifs: $(docker ps --filter ancestor=python --format '{{.Names}}' | wc -l)"
echo "Images Python: $(docker images --filter reference='python*' --format '{{.Repository}}:{{.Tag}}' | wc -l)"
echo "Espace disque /tmp: $(du -sh /tmp 2>/dev/null)"

# V√©rifier la sant√© de Docker
if ! docker info >/dev/null 2>&1; then
    echo "ALERTE: Docker n'est pas accessible"
    exit 1
fi

echo "Maintenance termin√©e avec succ√®s"
```

#### Monitoring avec Prometheus (optionnel)

```yaml
# docker-compose.yml pour monitoring
version: '3.8'
services:
  node-exporter:
    image: prom/node-exporter
    ports:
      - "9100:9100"
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /:/rootfs:ro
    command:
      - '--path.procfs=/host/proc'
      - '--path.sysfs=/host/sys'
      - '--collector.filesystem.ignored-mount-points'
      - '^/(sys|proc|dev|host|etc|rootfs/var/lib/docker/containers|rootfs/var/lib/docker/overlay2|rootfs/run/docker/netns|rootfs/var/lib/docker/aufs)($|/)'

  cadvisor:
    image: gcr.io/cadvisor/cadvisor
    ports:
      - "8080:8080"
    volumes:
      - /:/rootfs:ro
      - /var/run:/var/run:ro
      - /sys:/sys:ro
      - /var/lib/docker/:/var/lib/docker:ro
      - /dev/disk/:/dev/disk:ro
```

## üìä M√©triques et KPIs

### M√©triques de performance

| M√©trique | Valeur normale | Alerte | Critique |
|----------|----------------|---------|----------|
| Temps construction image | < 30s | > 60s | > 120s |
| Temps ex√©cution Python | < 10s | > 30s | > timeout |
| Utilisation m√©moire | < 512MB | > 1GB | > 2GB |
| Espace disque /tmp | < 1GB | > 5GB | > 10GB |
| Nombre conteneurs actifs | < 5 | > 20 | > 50 |

### Indicateurs de qualit√©

```python
# Code de monitoring int√©gr√©
import psutil
import time

# Mesures de performance dans le script Python
start_time = time.time()
memory_start = psutil.Process().memory_info().rss / 1024 / 1024  # MB

# ... votre code Python ...

execution_time = time.time() - start_time
memory_peak = psutil.Process().memory_info().rss / 1024 / 1024  # MB

additional_data = {
    'performance': {
        'execution_time_seconds': round(execution_time, 3),
        'memory_usage_mb': round(memory_peak, 2),
        'memory_delta_mb': round(memory_peak - memory_start, 2)
    }
}
```

## üö® Alerting et incidents

### Configuration d'alertes

```javascript
// Dans le flow Node-RED - n≈ìud de monitoring
if (msg.executionTime > 30000) {  // > 30 secondes
    node.warn(`Performance d√©grad√©e: ${msg.executionTime}ms`);
}

if (msg.pythonErrors && msg.pythonErrors.includes('OutOfMemoryError')) {
    node.error('Erreur m√©moire d√©tect√©e - v√©rifier les limites');
}

// Compteur d'erreurs
context.set('error_count', (context.get('error_count') || 0) + 1);
if (context.get('error_count') > 5) {
    node.error('Trop d\'erreurs cons√©cutives - intervention requise');
}
```

### Proc√©dures de r√©cup√©ration

#### R√©cup√©ration automatique
```javascript
// Auto-retry avec backoff exponentiel
let retryCount = msg.retryCount || 0;
if (retryCount < 3) {
    setTimeout(() => {
        msg.retryCount = retryCount + 1;
        node.send(msg);  // Retry
    }, Math.pow(2, retryCount) * 1000);  // 1s, 2s, 4s
}
```

#### Proc√©dure manuelle d'urgence
```bash
#!/bin/bash
# emergency_cleanup.sh

echo "=== PROC√âDURE D'URGENCE ==="

# Arr√™ter tous les conteneurs Python
docker ps --filter ancestor=python --format '{{.ID}}' | xargs -r docker kill
echo "Conteneurs Python arr√™t√©s"

# Nettoyer tout le cache
docker system prune -af --volumes
echo "Cache Docker nettoy√©"

# Nettoyer /tmp
rm -rf /tmp/python-exec-*
echo "Workspaces nettoy√©s"

# Red√©marrer Docker si n√©cessaire
if ! docker info >/dev/null 2>&1; then
    sudo systemctl restart docker
    echo "Docker red√©marr√©"
fi

# Red√©marrer Node-RED
sudo systemctl restart node-red
echo "Node-RED red√©marr√©"

echo "Proc√©dure d'urgence termin√©e"
```

## üìà √âvolutions et roadmap

### Fonctionnalit√©s pr√©vues

#### Version 2.0
- **Multi-langage** : Support R, Julia, Rust
- **Orchestration** : Kubernetes backend
- **Cache distribu√©** : Redis pour cache inter-instances
- **Streaming** : Support des gros datasets

#### Version 2.1
- **GPU Support** : Conteneurs avec CUDA
- **Notebooks** : Int√©gration Jupyter
- **Debugging** : Mode interactif
- **Templates** : Biblioth√®que de scripts pr√©-configur√©s

### Migration et compatibilit√©

```javascript
// Migration automatique des anciennes configurations
function migrateConfig(oldConfig) {
    const newConfig = { ...oldConfig };
    
    // Migration v1.0 -> v2.0
    if (!newConfig.version || newConfig.version < '2.0') {
        newConfig.reuseContainer = false;  // Valeur par d√©faut
        newConfig.maxMemory = '512m';
        newConfig.version = '2.0';
    }
    
    return newConfig;
}
```

## ü§ù Contribution et support

### Signaler un bug

```markdown
**Template de bug report**

**Version du module** : 1.0.0
**Version Node-RED** : 3.0.2
**Version Docker** : 20.10.17
**OS** : Ubuntu 22.04

**Description du probl√®me** :
[Description claire et concise]

**√âtapes pour reproduire** :
1. Configurer le n≈ìud avec...
2. Envoyer un message avec...
3. Observer l'erreur...

**Comportement attendu** :
[Ce qui devrait se passer]

**Logs** :
```
[Logs Node-RED et Docker]
```

**Configuration** :
```json
{
  "pythonLibraries": "...",
  "containerImage": "...",
  // ...
}
```
```

### Demande de fonctionnalit√©

```markdown
**Template de feature request**

**Probl√®me √† r√©soudre** :
[Quel probl√®me cette fonctionnalit√© r√©soudrait-elle ?]

**Solution propos√©e** :
[Description de la solution id√©ale]

**Alternatives consid√©r√©es** :
[Autres approches envisag√©es]

**Impact** :
- Performance : [Impact estim√©]
- S√©curit√© : [Consid√©rations de s√©curit√©]
- Compatibilit√© : [Breaking changes ?]
```

### D√©veloppement local

```bash
# Setup d√©veloppement
git clone https://github.com/your-repo/node-red-python-container
cd node-red-python-container

# Installation
npm install

# Tests unitaires
npm test

# Tests d'int√©gration
npm run test:integration

# Linting
npm run lint

# Build
npm run build
```

---

## üìù Changelog

### Version 1.0.0 (Actuelle)
- ‚úÖ Ex√©cution Python isol√©e dans Docker
- ‚úÖ Installation dynamique de d√©pendances
- ‚úÖ Interface graphique avec √©diteur ACE
- ‚úÖ Gestion asynchrone compl√®te
- ‚úÖ Fonctions utilitaires Python
- ‚úÖ Gestion d'erreurs robuste
- ‚úÖ Cache de conteneurs optionnel
- ‚úÖ Limites de ressources configurables

### Version 0.9.0 (Beta)
- ‚úÖ Proof of concept fonctionnel
- ‚úÖ Ex√©cution synchrone de base
- ‚ö†Ô∏è Probl√®mes de performance
- ‚ö†Ô∏è Gestion d'erreurs limit√©e

---

**Auteurs** : √âquipe de d√©veloppement Node-RED Python Container
**License** : MIT
**Documentation mise √† jour** : Janvier 2025

---

*Ce README constitue la documentation technique compl√®te du module Python Container pour Node-RED. Pour toute question ou probl√®me, consultez la section d√©pannage ou cr√©ez une issue sur le repository du projet.*